{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "southwest-footage",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "![logo](../picture/license_header_logo.png)\n",
    "\n",
    "> **Copyright (c) 2020-2021 CertifAI Sdn. Bhd.**<br>\n",
    " <br>\n",
    "This program is part of OSRFramework. You can redistribute it and/or modify\n",
    "<br>it under the terms of the GNU Affero General Public License as published by\n",
    "<br>the Free Software Foundation, either version 3 of the License, or\n",
    "<br>(at your option) any later version.\n",
    "<br>\n",
    "<br>This program is distributed in the hope that it will be useful,\n",
    "<br>but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "<br>MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "<br>GNU Affero General Public License for more details.\n",
    "<br>\n",
    "<br>You should have received a copy of the GNU Affero General Public License\n",
    "<br>along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb38a88",
   "metadata": {},
   "source": [
    "Authored by: [BK Yeoh](boonkhai.yeoh@certifai.ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde3a65",
   "metadata": {},
   "source": [
    "# Model Serialization & Model Conversion\n",
    "## Introduction\n",
    "This tutorial is to demonstrate the ways of model serialization and model conversion. It consists of 2 major parts which are:\n",
    "1. Demonstration of Model Serialization\n",
    "2. Demonstration of Model Conversion\n",
    "\n",
    "## What is serialization?\n",
    "It is a saving procedure that uses to represent an object with a stream of bytes. The goal is to save the model's parameters and coefficients to a file so that the model training and parameter optimization steps do not have to be repeated on new data to apply during the production.\n",
    "\n",
    ">The most common method is to serialize the model using some particular format after training and deserialize that model in the production environment.\n",
    "\n",
    "## What is model conversion? \n",
    "Model conversion is a promising technology for improving framework interoperability that transforms a source model into  another target framework format.\n",
    "\n",
    "## Notebook Outline\n",
    "Below is the outline for this tutorial:\n",
    "* [Helper Function](#Helper)\n",
    "* [Load Model](#load)\n",
    "* [Image transform](#transform)\n",
    "* [Inference Function](#Inference)\n",
    "* [Load Model State Dict](#Load)\n",
    "* [Ways of (de)serialize model](#Ways)\n",
    "    * [Save Model using Python Pickle](#Pickle)\n",
    "    * [Save Model using Joblib](#Joblib)\n",
    "    * [Save model using TorchScript](#TorchScript)\n",
    "* [Model Conversion](#Conversion)\n",
    "    * [Save Model in ONNX format](#ONNXformat)\n",
    "    * [Save Model in Protocol Buffers](#ProtocolBuffers)\n",
    "    * [Save model in Json format](#Jsonformat)\n",
    "* [Tips for Saving Your Model](#Tips)\n",
    "* [Summary](#Summary)\n",
    "* [Reference](#Reference)\n",
    "\n",
    "## What will we accomplish?\n",
    "1. Understand different ways of model serialization. \n",
    "2. Understand the process of model conversion and usage of each file format.\n",
    "\n",
    "First, let's import the package needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beginning-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "from torchvision import transforms,datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from time import time\n",
    "import warnings\n",
    "import utils "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3738994",
   "metadata": {},
   "source": [
    "## Helper Function <a id=Helper></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e89cf3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory generated_model/demo already exists, skipping create\n",
      "Subfolder pickle already exists, skipping create\n",
      "Subfolder joblib already exists, skipping create\n",
      "Subfolder torchscript already exists, skipping create\n",
      "Subfolder onnx already exists, skipping create\n",
      "Subfolder protocol_buffers already exists, skipping create\n",
      "Subfolder json already exists, skipping create\n"
     ]
    }
   ],
   "source": [
    "# Generate folder to store the model\n",
    "gen_model_path = 'generated_model/demo'\n",
    "dir_list = ['pickle','joblib','torchscript','onnx','protocol_buffers','json'] \n",
    "utils.folder_generator(gen_model_path,dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7b82f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "# Download the data from wasabisys         \n",
    "source = 'https://s3.eu-central-1.wasabisys.com/certifai/deployment-training-labs/fruits_image_classification-20210604T123547Z-001.zip'\n",
    "target = '../resources/data/'\n",
    "filename = 'fruits_image_classification.zip'\n",
    "utils.download(source, target, filename,zip_file = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54020be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "# Download the model from wasabisys \n",
    "model_url = \"https://s3.eu-central-1.wasabisys.com/certifai/deployment-training-labs/models/fruit_classifier_state_dict.pt\"\n",
    "modelname = \"fruit_classifier_state_dict.pt\"\n",
    "utils.download(model_url, gen_model_path, modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35be4f8c",
   "metadata": {},
   "source": [
    "First, let get the dirty test dataset folder path that you have downloaded in the previous lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "explicit-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is located in the data folder\n",
    "datadir = Path().resolve().parent/'resources/data/'\n",
    "dirtytestdir = datadir/'fruits_image_classification/dirty_test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad0cb79",
   "metadata": {},
   "source": [
    "## Load Model <a id=load></a>\n",
    "Download the serialized model in the previous lesson. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660fec09",
   "metadata": {},
   "source": [
    "Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hollywood-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(18496, 120) # Note that the input of this layers is depending on your input image sizes \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 3) \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu1(self.conv1(x)))\n",
    "        x = self.pool(self.relu2(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622007f5",
   "metadata": {},
   "source": [
    "## Image transform<a id=transform></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "incoming-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need our input in tensors form, must `transforms.ToTensor(),`\n",
    "val_transform = transforms.Compose([\n",
    "        transforms.Resize(150), # You can set to higher resolution for better result\n",
    "        transforms.CenterCrop(150),\n",
    "        transforms.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e4c2c",
   "metadata": {},
   "source": [
    "## Inference Function<a id=Inference></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "furnished-lender",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(best_model, testdir = dirtytestdir , image_transforms = val_transform):\n",
    "    best_model.eval()\n",
    "    test_data = datasets.ImageFolder(root = testdir, transform = image_transforms)\n",
    "    testloader = DataLoader(test_data, len(test_data), shuffle=False)\n",
    "    for images, labels in testloader:\n",
    "        correct = 0\n",
    "        y_pred = best_model(images)\n",
    "        predictions = torch.max(y_pred, 1)[1]\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        accuracy = correct / len(test_data) \n",
    "    print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26faf325",
   "metadata": {},
   "source": [
    "## Load Model State Dict<a id=Load></a>\n",
    "### What is a state_dict?\n",
    "* It is a Python dictionary object that maps each layer to its parameter tensor.\n",
    "* When saving the model for inference, saving the model’s state_dict will give the flexibility for restoring the model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "friendly-computer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (fc1): Linear(in_features=18496, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=3, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To load a saved state dict, we need to instantiate the model first\n",
    "model = Net()\n",
    "\n",
    "# Notice that the load_state_dict() function takes a dictionary object, NOT a path to a saved object. \n",
    "# This means that you must deserialize the saved state_dict before you pass it to the load_state_dict() function.\n",
    "model.load_state_dict(torch.load(os.path.join(gen_model_path,'fruit_classifier_state_dict.pt')))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f8f96e",
   "metadata": {},
   "source": [
    "## Ways of (de)serialize model<a id=Ways></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-harrison",
   "metadata": {},
   "source": [
    "### Save Model using Python Pickle<a id=Pickle></a>\n",
    "#### What is Python Pickle\n",
    "* It’s a standard Python tool to saving object (serialization) into a bytes file\n",
    "* The object can be a model or a data\n",
    "\n",
    "#### Advantages of using Python Pickle \n",
    "* Quicksave and restore learning model\n",
    "\n",
    "#### The downside of Python Pickle \n",
    "* Does not have security\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sophisticated-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "liberal-defendant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for saving model with pickle => 0.00899815559387207\n"
     ]
    }
   ],
   "source": [
    "# save the model to disk\n",
    "pickle_wt = time()\n",
    "pickle_filename = os.path.join(gen_model_path,'pickle/fruit_classifier.pt')\n",
    "pickle.dump(model, open(pickle_filename, 'wb'))\n",
    "print(\"Time for saving model with pickle =>\", time()-pickle_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "possible-leave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for loading file size with pickle 8936493 KB => 0.013002157211303711\n",
      "Test Accuracy: 0.7222222222222222\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "pickle_rt = time()\n",
    "loaded_model_pk = pickle.load(open(pickle_filename, 'rb'))\n",
    "print(\"Time for loading file size with pickle\", os.path.getsize(pickle_filename),\"KB =>\", time()-pickle_rt)\n",
    "inference(loaded_model_pk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-parker",
   "metadata": {},
   "source": [
    "### Save Model using Joblib<a id=Joblib></a>\n",
    "#### What is Joblib\n",
    "Joblib is part of the SciPy ecosystem and provides utilities for pipelining Python jobs.\n",
    "It provides utilities for saving and loading Python objects that make use of NumPy data structures, efficiently.\n",
    "This can be useful for some machine learning algorithms that require a lot of parameters or store the entire dataset\n",
    "\n",
    "#### Advantages of using Joblib \n",
    "* The Joblib library is intended to be a replacement for Pickle by offers a bit simpler workflow compared to Pickle.\n",
    "* It works with both file objects and string filenames.\n",
    "* If the model contains large data arrays, each array will be stored in its own file, but the save and restore procedure will remain the same.\n",
    "* Joblib also supports various compression methods, such as 'zlib,' 'gzip,' and 'bz2', as well as different levels of compression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "willing-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "metropolitan-brooks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for saving model with joblib => 0.009999990463256836\n"
     ]
    }
   ],
   "source": [
    "# save the model to disk\n",
    "joblib_wt = time()\n",
    "joblib_filename = os.path.join(gen_model_path,'joblib/fruit_classifier.pt')\n",
    "joblib.dump(model, joblib_filename)\n",
    "print(\"Time for saving model with joblib =>\", time()-joblib_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ignored-skating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for loading file size with joblib 8936493 KB => 0.014000654220581055\n",
      "Test Accuracy: 0.7222222222222222\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "joblib_rt = time()\n",
    "loaded_model_jb = joblib.load(joblib_filename)\n",
    "print(\"Time for loading file size with joblib\", os.path.getsize(joblib_filename),\"KB =>\", time()-joblib_rt)\n",
    "inference(loaded_model_jb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c84feb",
   "metadata": {},
   "source": [
    "### Biggest Drawback of Pickle and Joblib\n",
    "#### Python version compatibility \n",
    "* Both tools are not recommended to (de)serialize objects across different Python versions, though it may work with minor version changes.\n",
    "\n",
    "#### Model compatibility\n",
    "* The internal structure of the model must remain constant between saving and reload for both tools.\n",
    "\n",
    "#### Security\n",
    "* Both tools may contain malicious code, so restoring data from untrusted or unauthenticated sources is not recommended.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e16404",
   "metadata": {},
   "source": [
    "### Save model using TorchScript <a id=TorchScript></a>\n",
    "\n",
    "#### PyTorch Ecosystem\n",
    "PyTorch has two distinct modes for dealing with the research and production environments.<br>\n",
    "\n",
    "**Eager mode** - It is designed to facilitate faster prototyping, training, and experimentation.<br>\n",
    "**Script mode** - It is geared toward the production use case. It is made up of two parts: `PyTorch JIT` and `TorchScript`.<br>\n",
    "\n",
    "<div><img  src=\"../picture/eager_to_script.png\",width=850,height=200></div>\n",
    "<center><b>Tools to transition from eager to script</b></center>\n",
    "\n",
    "#### What is script mode?\n",
    "Script mode creates an intermediate representation (IR) from the PyTorch Eager module. The IR is internally optimized and utilizes PyTorch JIT compilation at runtime.\n",
    "<div><img  src=\"../picture/script_mode.png\",width=850,height=200></div>\n",
    "<center><b>Details of script mode</b></center>\n",
    "\n",
    "#### What is PyTorch JIT\n",
    "It is a compiler and language infrastructure for machine learning and an optimized compiler for PyTorch programs.\n",
    "1. It is a lightweight threadsafe interpreter\n",
    "2. Supports easy to write custom transformations\n",
    "3. It’s not just for inference as it has auto diff support\n",
    "\n",
    "#### What is TorchScript?\n",
    "TorchScript is an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++.\n",
    "\n",
    ">Torch Script is a way to create serializable and optimizable models from PyTorch code. Any code written in Torch Script can be saved from your Python process and loaded in a process where there is no Python dependency. - *Pytorch.org*\n",
    "\n",
    "TorchScript is a static high-performance subset of Python language, specialized for ML applications. It supports\n",
    "1. Complex control flows\n",
    "2. Common data structures\n",
    "3. User-defined classes\n",
    "\n",
    "#### Why Script mode?\n",
    "1. **Portability**<br>\n",
    "Portability enables models to be deployed in multithreaded inference servers, mobile devices, and automobiles which difficult due to the tight coupling of models to the Python runtime. \n",
    "\n",
    "2. **Performance**<br>\n",
    "Optimize common patterns in neural networks to improve inference latency. There is numerous further optimization that cannot achieve with the level of dynamism in the Python language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dea6b2",
   "metadata": {},
   "source": [
    "#### Save model using `JIT Trace`\n",
    "`torch.jit.trace` take a data instance and your trained eager module as input. The tracer runs the supplied module and records the tensor operations performed. This recording is turned into a TorchScript module.\n",
    "\n",
    "Drawbacks\n",
    "it omits: \n",
    "- control flow,\n",
    "- data structures\n",
    "- python constructs\n",
    "- create unfaithful representations without any warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3144d2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  original_name=Net\n",
      "  (conv1): Conv2d(original_name=Conv2d)\n",
      "  (relu1): ReLU(original_name=ReLU)\n",
      "  (pool): MaxPool2d(original_name=MaxPool2d)\n",
      "  (conv2): Conv2d(original_name=Conv2d)\n",
      "  (relu2): ReLU(original_name=ReLU)\n",
      "  (fc1): Linear(original_name=Linear)\n",
      "  (fc2): Linear(original_name=Linear)\n",
      "  (fc3): Linear(original_name=Linear)\n",
      "  (relu): ReLU(original_name=ReLU)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build an input example (batch size, channels, image height, image width) which image size is set in image transform \n",
    "x = torch.randn(10, 3, 150, 150, requires_grad=True)\n",
    "traced_model = torch.jit.trace(model,x)\n",
    "print(traced_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f44c90c",
   "metadata": {},
   "source": [
    "TorchScript records its definitions in an Intermediate Representation (or IR), commonly referred to in Deep learning as a graph. We can examine the graph with the `.graph` property:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf9b718f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self.1 : __torch__.Net,\n",
      "      %input.1 : Float(10, 3, 150, 150, strides=[67500, 22500, 150, 1], requires_grad=1, device=cpu)):\n",
      "  %155 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name=\"fc3\"](%self.1)\n",
      "  %152 : __torch__.torch.nn.modules.linear.___torch_mangle_2.Linear = prim::GetAttr[name=\"fc2\"](%self.1)\n",
      "  %149 : __torch__.torch.nn.modules.activation.___torch_mangle_4.ReLU = prim::GetAttr[name=\"relu\"](%self.1)\n",
      "  %148 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"fc1\"](%self.1)\n",
      "  %145 : __torch__.torch.nn.modules.activation.___torch_mangle_1.ReLU = prim::GetAttr[name=\"relu2\"](%self.1)\n",
      "  %144 : __torch__.torch.nn.modules.conv.___torch_mangle_0.Conv2d = prim::GetAttr[name=\"conv2\"](%self.1)\n",
      "  %141 : __torch__.torch.nn.modules.pooling.MaxPool2d = prim::GetAttr[name=\"pool\"](%self.1)\n",
      "  %140 : __torch__.torch.nn.modules.activation.ReLU = prim::GetAttr[name=\"relu1\"](%self.1)\n",
      "  %139 : __torch__.torch.nn.modules.conv.Conv2d = prim::GetAttr[name=\"conv1\"](%self.1)\n",
      "  %167 : Tensor = prim::CallMethod[name=\"forward\"](%139, %input.1)\n",
      "  %168 : Tensor = prim::CallMethod[name=\"forward\"](%140, %167)\n",
      "  %169 : Tensor = prim::CallMethod[name=\"forward\"](%141, %168)\n",
      "  %170 : Tensor = prim::CallMethod[name=\"forward\"](%144, %169)\n",
      "  %171 : Tensor = prim::CallMethod[name=\"forward\"](%145, %170)\n",
      "  %172 : Tensor = prim::CallMethod[name=\"forward1\"](%141, %171)\n",
      "  %109 : int = prim::Constant[value=1]() # <ipython-input-6-564397efff52>:17:0\n",
      "  %110 : int = prim::Constant[value=-1]() # <ipython-input-6-564397efff52>:17:0\n",
      "  %input.7 : Float(10, 18496, strides=[18496, 1], requires_grad=1, device=cpu) = aten::flatten(%172, %109, %110) # <ipython-input-6-564397efff52>:17:0\n",
      "  %173 : Tensor = prim::CallMethod[name=\"forward\"](%148, %input.7)\n",
      "  %174 : Tensor = prim::CallMethod[name=\"forward\"](%149, %173)\n",
      "  %175 : Tensor = prim::CallMethod[name=\"forward\"](%152, %174)\n",
      "  %176 : Tensor = prim::CallMethod[name=\"forward1\"](%149, %175)\n",
      "  %177 : Tensor = prim::CallMethod[name=\"forward\"](%155, %176)\n",
      "  return (%177)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Intermediate representation (IR) of the model\n",
    "print(traced_model.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4133731a",
   "metadata": {},
   "source": [
    "However, this is a very low-level representation and most of the information contained in the graph is not useful for end users. Instead, we can use the `.code` property to give a Python-syntax interpretation of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b66b5c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    input: Tensor) -> Tensor:\n",
      "  _0 = self.fc3\n",
      "  _1 = self.fc2\n",
      "  _2 = self.relu\n",
      "  _3 = self.fc1\n",
      "  _4 = self.relu2\n",
      "  _5 = self.conv2\n",
      "  _6 = self.pool\n",
      "  _7 = (self.relu1).forward((self.conv1).forward(input, ), )\n",
      "  _8 = (_4).forward((_5).forward((_6).forward(_7, ), ), )\n",
      "  input0 = torch.flatten((_6).forward1(_8, ), 1, -1)\n",
      "  _9 = (_2).forward((_3).forward(input0, ), )\n",
      "  _10 = (_0).forward((_2).forward1((_1).forward(_9, ), ), )\n",
      "  return _10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(traced_model.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63f5386",
   "metadata": {},
   "source": [
    "##### (De)serialize `traced_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ccdfb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize model to disk\n",
    "torch_path = os.path.join(gen_model_path,\"torchscript/\")\n",
    "torch.jit.save(traced_model,torch_path +\"traced_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88ae42fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7222222222222222\n"
     ]
    }
   ],
   "source": [
    "# Deserialize model from disk\n",
    "loaded_traced_model = torch.jit.load(torch_path +\"traced_model.pt\")\n",
    "inference(loaded_traced_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5b5762",
   "metadata": {},
   "source": [
    "#### Save model using `JIT Script`\n",
    "Unfortunately, things like control flow are erased by using `torch.jit.trace`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95f27deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionGate(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        if x.sum() > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return -x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a203eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = MyDecisionGate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dfe2c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor) -> Tensor:\n",
      "  return torch.neg(x)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "traced_new_model = torch.jit.trace(new_model,x)\n",
    "print(traced_new_model.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294df6c0",
   "metadata": {},
   "source": [
    "How can we faithfully represent this module in TorchScript? We provide a script compiler, which does direct analysis of your Python source code to transform it into TorchScript. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8611a07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor) -> Tensor:\n",
      "  _0 = bool(torch.gt(torch.sum(x, dtype=None), 0))\n",
      "  if _0:\n",
      "    _1 = x\n",
      "  else:\n",
      "    _1 = torch.neg(x)\n",
      "  return _1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scripted_gate = torch.jit.script(new_model)\n",
    "print(scripted_gate.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd59354",
   "metadata": {},
   "source": [
    "##### (De)serialize `script_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f788b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_model = torch.jit.script(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85d42265",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_path = os.path.join(gen_model_path,\"torchscript/\")\n",
    "torch.jit.save(script_model,torch_path +\"script_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c13e254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7222222222222222\n"
     ]
    }
   ],
   "source": [
    "loaded_script_model = torch.jit.load(torch_path +\"script_model.pt\")\n",
    "inference(loaded_script_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae2c1c",
   "metadata": {},
   "source": [
    "### Advantages of using TorchScript\n",
    "* TorchScript decouples your model from any runtime environment, improving inference time. It eliminates Python's GIL, which is a significant bottleneck for multithreaded inference.\n",
    "* Focusing on optimising entire programmes.\n",
    "* It optimises common neural network patterns automatically to improve latency and throughput.\n",
    "TorchScript modules can be exported to a wide range of environments, from C++ servers to mobile devices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-protest",
   "metadata": {},
   "source": [
    "# Model Conversion<a id=Conversion></a>\n",
    "We can convert the model from the PyTorch framework to others such as TensorFlow and Keras. These can achieve by using the model converters from the official website or third-party package.\n",
    "\n",
    "Example of the third-party package converter:\n",
    "\n",
    "converter|\tmxnet|\tcaffe| \tcaffe2|\tCNTK|\ttheano/lasagne|\tneon|\tpytorch|\ttorch|\tkeras| \tdarknet|\ttensorflow| \tchainer|\tcoreML/iOS|\n",
    "---|\t---|\t---| \t---|\t---|\t---|\t---|\t---|\t---|\t---| \t---|\t---| \t---|\t---\n",
    "pytorch| \t[MMdnn](https://github.com/Microsoft/MMdnn)|\t[MMdnn](https://github.com/Microsoft/MMdnn) [pytorch2caffe](https://github.com/longcw/pytorch2caffe) [pytorch-caffe-darknet-convert](https://github.com/nanhui69/pytorch-caffe-darknet-convert)|\tPart of Pytorch now|\tONNX [MMdnn](https://github.com/Microsoft/MMdnn)|\tNone|\tNone|\t-|\tNone|\t[MMdnn]((https://github.com/Microsoft/MMdnn)) [pytorch2keras](https://github.com/gmalivenko/pytorch2keras) [onnx2keras](https://github.com/waltYeh/onnx2keras)|\tNone| [onnx-tf](https://github.com/onnx/onnx-tensorflow)|\tNone|\t[onnx-coreml](https://github.com/onnx/onnx-coreml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-highlight",
   "metadata": {},
   "source": [
    "## Save Model in ONNX format<a id=ONNXformat></a>\n",
    "PyTorch also supports saving model as ONNX (Open Neural Network Exchange) file type, which is an open format built to represent machine learning models. ONNX provides a standard that would allow portability and interoperability between the already well-known Deep Learning frameworks like PyTorch, Tensorflow, Caffe2, MXNet, etc. For example, we can train the model in one framework and export the model in ONNX format to perform inference in another framework.\n",
    "Also, we can utilise the ONNX Runtime to accelerate inference and allow the model to run in different hardware architectures.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dense-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an input example (batch size, channels, image height, image width) which image size is set in image transform \n",
    "# As the argument in torch.onnx.export\n",
    "x = torch.randn(10, 3, 150, 150, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "spanish-bracelet",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input : Float(10, 3, 150, 150, strides=[67500, 22500, 150, 1], requires_grad=1, device=cpu),\n",
      "      %conv1.weight : Float(6, 3, 5, 5, strides=[75, 25, 5, 1], requires_grad=1, device=cpu),\n",
      "      %conv1.bias : Float(6, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2.weight : Float(16, 6, 5, 5, strides=[150, 25, 5, 1], requires_grad=1, device=cpu),\n",
      "      %conv2.bias : Float(16, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc1.weight : Float(120, 18496, strides=[18496, 1], requires_grad=1, device=cpu),\n",
      "      %fc1.bias : Float(120, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc2.weight : Float(84, 120, strides=[120, 1], requires_grad=1, device=cpu),\n",
      "      %fc2.bias : Float(84, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc3.weight : Float(3, 84, strides=[84, 1], requires_grad=1, device=cpu),\n",
      "      %fc3.bias : Float(3, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %11 : Float(10, 6, 146, 146, strides=[127896, 21316, 146, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%input, %conv1.weight, %conv1.bias) # C:\\Users\\Jacklyn.Lim\\miniconda3\\envs\\deploy\\lib\\site-packages\\torch\\nn\\modules\\conv.py:395:0\n",
      "  %12 : Float(10, 6, 146, 146, strides=[127896, 21316, 146, 1], requires_grad=1, device=cpu) = onnx::Relu(%11) # C:\\Users\\Jacklyn.Lim\\miniconda3\\envs\\deploy\\lib\\site-packages\\torch\\nn\\functional.py:1206:0\n",
      "  %13 : Float(10, 6, 73, 73, strides=[31974, 5329, 73, 1], requires_grad=1, device=cpu) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%12) # C:\\Users\\Jacklyn.Lim\\miniconda3\\envs\\deploy\\lib\\site-packages\\torch\\nn\\functional.py:659:0\n",
      "  %14 : Float(10, 16, 69, 69, strides=[76176, 4761, 69, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%13, %conv2.weight, %conv2.bias) # C:\\Users\\Jacklyn.Lim\\miniconda3\\envs\\deploy\\lib\\site-packages\\torch\\nn\\modules\\conv.py:395:0\n",
      "  %15 : Float(10, 16, 69, 69, strides=[76176, 4761, 69, 1], requires_grad=1, device=cpu) = onnx::Relu(%14) # C:\\Users\\Jacklyn.Lim\\miniconda3\\envs\\deploy\\lib\\site-packages\\torch\\nn\\functional.py:1206:0\n",
      "  %16 : Float(10, 16, 34, 34, strides=[18496, 1156, 34, 1], requires_grad=1, device=cpu) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%15) # C:\\Users\\Jacklyn.Lim\\miniconda3\\envs\\deploy\\lib\\site-packages\\torch\\nn\\functional.py:659:0\n",
      "  %17 : Float(10, 18496, strides=[18496, 1], requires_grad=1, device=cpu) = onnx::Flatten[axis=1](%16) # <ipython-input-6-564397efff52>:17:0\n",
      "  %18 : Float(10, 120, strides=[120, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%17, %fc1.weight, %fc1.bias) # C:\\Users\\Jacklyn.Lim\\miniconda3\\envs\\deploy\\lib\\site-packages\\torch\\nn\\functional.py:1753:0\n",
      "  %19 : Float(10, 120, strides=[120, 1], requires_grad=1, device=cpu) = onnx::Relu(%18) # C:\\Users\\Jacklyn.Lim\\miniconda3\\envs\\deploy\\lib\\site-packages\\torch\\nn\\functional.py:1206:0\n",
      "  %20 : Float(10, 84, strides=[84, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%19, %fc2.weight, %fc2.bias) # C:\\Users\\Jacklyn.Lim\\miniconda3\\envs\\deploy\\lib\\site-packages\\torch\\nn\\functional.py:1753:0\n",
      "  %21 : Float(10, 84, strides=[84, 1], requires_grad=1, device=cpu) = onnx::Relu(%20) # C:\\Users\\Jacklyn.Lim\\miniconda3\\envs\\deploy\\lib\\site-packages\\torch\\nn\\functional.py:1206:0\n",
      "  %output : Float(10, 3, strides=[3, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%21, %fc3.weight, %fc3.bias) # C:\\Users\\Jacklyn.Lim\\miniconda3\\envs\\deploy\\lib\\site-packages\\torch\\nn\\functional.py:1753:0\n",
      "  return (%output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Export the model\n",
    "onnx_filepath = os.path.join(gen_model_path,'onnx/fruit_classifier.onnx')\n",
    "torch.onnx.export(model, x,onnx_filepath, \n",
    "                  verbose = True, input_names = ['input'], output_names = ['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-spare",
   "metadata": {},
   "source": [
    "## Save Model in Protocol Buffers<a id=ProtocolBuffers></a>\n",
    "To save the model into protocol buffers `.pb` format. You are required to convert the model from ONNX to TensorFlow by using `onnx_tf`\n",
    "#### Procedure for  `onnx_tf ` installation\n",
    "Install `onnx_tf`  master branch from source.\n",
    "1. Install `TensorFlow >= 2.4.1` and `tensorflow-addons==0.13.0` using the following commands.\n",
    "\n",
    "    `pip install tensorflow==2.5.0` <br>\n",
    "    `pip install tensorflow-addons==0.13.0` <br>\n",
    "\n",
    "2. Run git clone https://github.com/onnx/onnx-tensorflow.git && `cd onnx-tensorflow`.\n",
    "3. Run `pip install -e .`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "successful-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx_tf.backend import prepare\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "daily-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ONNX model format and convert to tensorflow model format \n",
    "onnx_model = onnx.load(onnx_filepath)\n",
    "tf_rep = prepare(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "attended-usage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: generated_model/demo\\protocol_buffers\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: generated_model/demo\\protocol_buffers\\assets\n"
     ]
    }
   ],
   "source": [
    "# Covert the tensorflow format to Protocol Buffers format\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pb_filepath = os.path.join(gen_model_path,\"protocol_buffers\")\n",
    "tf_rep.export_graph(pb_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff784f",
   "metadata": {},
   "source": [
    "Run the below code to validate the Protocol Buffers format The input shape should be the same as the input when serializing the model in ONNX format which is `[10,3,150,150]` and the output shape should be the expected output shape base on the model's configuration which is `[10,3]`\n",
    "![image](https://user-images.githubusercontent.com/59526258/121812655-3f5d2c00-cc9b-11eb-9062-b4b9c73ce91b.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "loved-minutes",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (10, 3, 150, 150)\n",
      "        name: serving_default_input:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['output'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (10, 3)\n",
      "        name: PartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "\n",
      "Defined Functions:\n",
      "  Function Name: '__call__'\n",
      "        Named Argument #1\n",
      "          input\n",
      "\n",
      "  Function Name: 'gen_tensor_dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-09 13:09:23.669529: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2021-08-09 13:09:23.669558: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir {pb_filepath} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-isolation",
   "metadata": {},
   "source": [
    "## Save model in Json format<a id=Jsonformat></a>\n",
    "To save the model into JSON `.json` format. You are required to convert the model from ONNX to Keras by using `onnx_to_keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "beneficial-bacteria",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:onnx2keras:maxpool:Unable to use `same` padding. Add ZeroPadding2D layer to fix shapes.\n",
      "WARNING:onnx2keras:maxpool:Unable to use `same` padding. Add ZeroPadding2D layer to fix shapes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 3, 150, 150)]     0         \n",
      "_________________________________________________________________\n",
      "11 (Conv2D)                  (None, 6, 146, 146)       456       \n",
      "_________________________________________________________________\n",
      "12 (Activation)              (None, 6, 146, 146)       0         \n",
      "_________________________________________________________________\n",
      "13 (MaxPooling2D)            (None, 6, 73, 73)         0         \n",
      "_________________________________________________________________\n",
      "14 (Conv2D)                  (None, 16, 69, 69)        2416      \n",
      "_________________________________________________________________\n",
      "15 (Activation)              (None, 16, 69, 69)        0         \n",
      "_________________________________________________________________\n",
      "16 (MaxPooling2D)            (None, 16, 34, 34)        0         \n",
      "_________________________________________________________________\n",
      "17 (Reshape)                 (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "18 (Dense)                   (None, 120)               2219640   \n",
      "_________________________________________________________________\n",
      "19 (Activation)              (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "20 (Dense)                   (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "21 (Activation)              (None, 84)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 3)                 255       \n",
      "=================================================================\n",
      "Total params: 2,232,931\n",
      "Trainable params: 2,232,931\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from onnx2keras import onnx_to_keras\n",
    "k_model = onnx_to_keras(onnx_model, ['input'])\n",
    "k_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "brazilian-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize model to JSON\n",
    "import json\n",
    "json_model_path = os.path.join(gen_model_path,'json')\n",
    "file_name = \"fruit_classifier.json\"\n",
    "complete_name = os.path.join(json_model_path,file_name)\n",
    "k_model_json = k_model.to_json()\n",
    "\n",
    "with open(complete_name, \"w\") as json_file:\n",
    "    json_file.write(k_model_json)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "toxic-titanium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_name': 'Functional',\n",
       " 'config': {'name': 'model',\n",
       "  'layers': [{'class_name': 'InputLayer',\n",
       "    'config': {'batch_input_shape': [None, 3, 150, 150],\n",
       "     'dtype': 'float32',\n",
       "     'sparse': False,\n",
       "     'ragged': False,\n",
       "     'name': 'input'},\n",
       "    'name': 'input',\n",
       "    'inbound_nodes': []},\n",
       "   {'class_name': 'Conv2D',\n",
       "    'config': {'name': '11',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'filters': 6,\n",
       "     'kernel_size': [5, 5],\n",
       "     'strides': [1, 1],\n",
       "     'padding': 'valid',\n",
       "     'data_format': 'channels_first',\n",
       "     'dilation_rate': [1, 1],\n",
       "     'groups': 1,\n",
       "     'activation': 'linear',\n",
       "     'use_bias': True,\n",
       "     'kernel_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'kernel_regularizer': None,\n",
       "     'bias_regularizer': None,\n",
       "     'activity_regularizer': None,\n",
       "     'kernel_constraint': None,\n",
       "     'bias_constraint': None},\n",
       "    'name': '11',\n",
       "    'inbound_nodes': [[['input', 0, 0, {}]]]},\n",
       "   {'class_name': 'Activation',\n",
       "    'config': {'name': '12',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'activation': 'relu'},\n",
       "    'name': '12',\n",
       "    'inbound_nodes': [[['11', 0, 0, {}]]]},\n",
       "   {'class_name': 'MaxPooling2D',\n",
       "    'config': {'name': '13',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'pool_size': [2, 2],\n",
       "     'padding': 'valid',\n",
       "     'strides': [2, 2],\n",
       "     'data_format': 'channels_first'},\n",
       "    'name': '13',\n",
       "    'inbound_nodes': [[['12', 0, 0, {}]]]},\n",
       "   {'class_name': 'Conv2D',\n",
       "    'config': {'name': '14',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'filters': 16,\n",
       "     'kernel_size': [5, 5],\n",
       "     'strides': [1, 1],\n",
       "     'padding': 'valid',\n",
       "     'data_format': 'channels_first',\n",
       "     'dilation_rate': [1, 1],\n",
       "     'groups': 1,\n",
       "     'activation': 'linear',\n",
       "     'use_bias': True,\n",
       "     'kernel_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'kernel_regularizer': None,\n",
       "     'bias_regularizer': None,\n",
       "     'activity_regularizer': None,\n",
       "     'kernel_constraint': None,\n",
       "     'bias_constraint': None},\n",
       "    'name': '14',\n",
       "    'inbound_nodes': [[['13', 0, 0, {}]]]},\n",
       "   {'class_name': 'Activation',\n",
       "    'config': {'name': '15',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'activation': 'relu'},\n",
       "    'name': '15',\n",
       "    'inbound_nodes': [[['14', 0, 0, {}]]]},\n",
       "   {'class_name': 'MaxPooling2D',\n",
       "    'config': {'name': '16',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'pool_size': [2, 2],\n",
       "     'padding': 'valid',\n",
       "     'strides': [2, 2],\n",
       "     'data_format': 'channels_first'},\n",
       "    'name': '16',\n",
       "    'inbound_nodes': [[['15', 0, 0, {}]]]},\n",
       "   {'class_name': 'Reshape',\n",
       "    'config': {'name': '17',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'target_shape': [-1]},\n",
       "    'name': '17',\n",
       "    'inbound_nodes': [[['16', 0, 0, {}]]]},\n",
       "   {'class_name': 'Dense',\n",
       "    'config': {'name': '18',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'units': 120,\n",
       "     'activation': 'linear',\n",
       "     'use_bias': True,\n",
       "     'kernel_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'kernel_regularizer': None,\n",
       "     'bias_regularizer': None,\n",
       "     'activity_regularizer': None,\n",
       "     'kernel_constraint': None,\n",
       "     'bias_constraint': None},\n",
       "    'name': '18',\n",
       "    'inbound_nodes': [[['17', 0, 0, {}]]]},\n",
       "   {'class_name': 'Activation',\n",
       "    'config': {'name': '19',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'activation': 'relu'},\n",
       "    'name': '19',\n",
       "    'inbound_nodes': [[['18', 0, 0, {}]]]},\n",
       "   {'class_name': 'Dense',\n",
       "    'config': {'name': '20',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'units': 84,\n",
       "     'activation': 'linear',\n",
       "     'use_bias': True,\n",
       "     'kernel_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'kernel_regularizer': None,\n",
       "     'bias_regularizer': None,\n",
       "     'activity_regularizer': None,\n",
       "     'kernel_constraint': None,\n",
       "     'bias_constraint': None},\n",
       "    'name': '20',\n",
       "    'inbound_nodes': [[['19', 0, 0, {}]]]},\n",
       "   {'class_name': 'Activation',\n",
       "    'config': {'name': '21',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'activation': 'relu'},\n",
       "    'name': '21',\n",
       "    'inbound_nodes': [[['20', 0, 0, {}]]]},\n",
       "   {'class_name': 'Dense',\n",
       "    'config': {'name': 'output',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'units': 3,\n",
       "     'activation': 'linear',\n",
       "     'use_bias': True,\n",
       "     'kernel_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'kernel_regularizer': None,\n",
       "     'bias_regularizer': None,\n",
       "     'activity_regularizer': None,\n",
       "     'kernel_constraint': None,\n",
       "     'bias_constraint': None},\n",
       "    'name': 'output',\n",
       "    'inbound_nodes': [[['21', 0, 0, {}]]]}],\n",
       "  'input_layers': [['input', 0, 0]],\n",
       "  'output_layers': [['output', 0, 0]]},\n",
       " 'keras_version': '2.5.0',\n",
       " 'backend': 'tensorflow'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the json file to validate the conversion\n",
    "json.load(open(complete_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "smoking-mobile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 3, 150, 150)]     0         \n",
      "_________________________________________________________________\n",
      "11 (Conv2D)                  (None, 6, 146, 146)       456       \n",
      "_________________________________________________________________\n",
      "12 (Activation)              (None, 6, 146, 146)       0         \n",
      "_________________________________________________________________\n",
      "13 (MaxPooling2D)            (None, 6, 73, 73)         0         \n",
      "_________________________________________________________________\n",
      "14 (Conv2D)                  (None, 16, 69, 69)        2416      \n",
      "_________________________________________________________________\n",
      "15 (Activation)              (None, 16, 69, 69)        0         \n",
      "_________________________________________________________________\n",
      "16 (MaxPooling2D)            (None, 16, 34, 34)        0         \n",
      "_________________________________________________________________\n",
      "17 (Reshape)                 (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "18 (Dense)                   (None, 120)               2219640   \n",
      "_________________________________________________________________\n",
      "19 (Activation)              (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "20 (Dense)                   (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "21 (Activation)              (None, 84)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 3)                 255       \n",
      "=================================================================\n",
      "Total params: 2,232,931\n",
      "Trainable params: 2,232,931\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# load json and create model\n",
    "json_file = open(complete_name, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-departure",
   "metadata": {},
   "source": [
    "## Tips for Saving Your Model<a id=Tips></a>\n",
    "This section discusses some important factors to consider when finishing your machine learning models according to Jason Brownlee.\n",
    "\n",
    "* **Python version**: Keep an eye out for the Python version. When load and deserialize the model, the same version of Python is needed that was used to serialise it.\n",
    "* **Library Editions**: When deserializing a saved model, the version of all major libraries used in machine learning project almost certainly needs to be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db44a1de",
   "metadata": {},
   "source": [
    "# Summary<a id=Summary></a>\n",
    "From this tutorial, you should have learned:\n",
    "1. Different methods of (de)serialize model.\n",
    "2. Model Conversion to different types of model format. \n",
    "\n",
    "Congratulations, that concludes this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-progress",
   "metadata": {},
   "source": [
    "# Reference<a id=Reference></a>\n",
    "1. [Machine Learning Mastery With Python - Jason Brownlee ](https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/)\n",
    "2. [Deep Learning Model Convertors](https://blog.csdn.net/baidu_40840693/article/details/86288667?ops_request_misc=&request_id=&biz_id=102&utm_term=pytorch%20model%20conversion&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-.first_rank_v2_pc_rank_v29&spm=1018.2226.3001.4187)\n",
    "3. [How to use gRPC API to Serve a Deep Learning Model?](https://towardsdatascience.com/serving-deep-learning-model-in-production-using-fast-and-efficient-grpc-6dfe94bf9234)\n",
    "4. [onnx2keras](https://github.com/waltYeh/onnx2keras)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deploy] *",
   "language": "python",
   "name": "conda-env-deploy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
