{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adolescent-proof",
   "metadata": {},
   "source": [
    "![logo](../../picture/license_header_logo.png)\n",
    "**Copyright (c) 2020-2021 CertifAI Sdn. Bhd.**\n",
    "\n",
    "This program is part of OSRFramework. You can redistribute it and/or modify\n",
    "it under the terms of the GNU Affero General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n",
    "GNU Affero General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Affero General Public License\n",
    "along with this program. If not, see http://www.gnu.org/licenses/.\n",
    "\n",
    "Authored by: [Jacklyn Lim](mailto:jacklyn.lim@certifai.ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-kenya",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "decent-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from utils import download_model, download_dataset, load_model_state_dict, load_dataset, load_image, inspect_module, compare_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-ensemble",
   "metadata": {},
   "source": [
    "### Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "electoral-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # Note that the input of this layers is depending on your input image sizes\n",
    "        self.fc1 = nn.Linear(18496, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-abuse",
   "metadata": {},
   "source": [
    "### Download and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "122e1ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://s3.eu-central-1.wasabisys.com/certifai/deployment-training-labs/models/fruit_classifier_state_dict.pt to ../../resources/model/\n",
      "100% [..........................................................................] 8935167 / 8935167\n",
      "Done!\n",
      "data already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "# model download\n",
    "MODEL_DOWNLOAD_PATH = 'https://s3.eu-central-1.wasabisys.com/certifai/deployment-training-labs/models/fruit_classifier_state_dict.pt'\n",
    "MODEL_STATE_DICT_PATH = '../../resources/model/'\n",
    "MODEL_FILENAME = 'fruits_image_classification.zip'\n",
    "download_model(MODEL_DOWNLOAD_PATH, MODEL_STATE_DICT_PATH, MODEL_FILENAME)\n",
    "\n",
    "# data download\n",
    "DATA_DOWNLOAD_PATH = \"https://s3.eu-central-1.wasabisys.com/certifai/deployment-training-labs/fruits_image_classification-20210604T123547Z-001.zip\"\n",
    "DATA_SAVE_PATH = \"../../resources/data/\"\n",
    "DATA_ZIP_FILENAME = \"fruits_image_classification.zip\"\n",
    "download_dataset(DATA_DOWNLOAD_PATH, DATA_SAVE_PATH, DATA_ZIP_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac662cfd",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "previous-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model = load_model_state_dict(model, MODEL_STATE_DICT_PATH + MODEL_FILENAME)\n",
    "\n",
    "original_model = copy.deepcopy(model)\n",
    "single_pruned_model = copy.deepcopy(model)\n",
    "globally_pruned_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-toner",
   "metadata": {},
   "source": [
    "### Iteratively Prune Model\n",
    "\n",
    "The only thing difference between single pruned model and iteratively pruned model is that in iteratively pruned models, the same parameter is pruned multiple times instead of just once.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fossil-highway",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_prune(model, modules, num_iterations, global_pruning = False):\n",
    "    \n",
    "    if (global_pruning):\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            prune.global_unstructured(\n",
    "                modules,\n",
    "                pruning_method=prune.L1Unstructured, \n",
    "                amount=0.3,\n",
    "            )\n",
    "    else:\n",
    "        for i in range(num_iterations):\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "geographic-repair",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# local single pruning\n",
    "module = single_pruned_model.conv1\n",
    "iterative_prune(single_pruned_model, module, num_iterations=5)\n",
    "\n",
    "# global pruning\n",
    "modules_to_prune = (\n",
    "    (globally_pruned_model.conv1, 'weight'),\n",
    "    (globally_pruned_model.conv2, 'weight'),\n",
    "    (globally_pruned_model.fc1, 'weight'),\n",
    "    (globally_pruned_model.fc2, 'weight'),\n",
    "    (globally_pruned_model.fc3, 'weight'),\n",
    ")\n",
    "iterative_prune(globally_pruned_model, modules_to_prune, num_iterations=5, global_pruning=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-value",
   "metadata": {},
   "source": [
    "The various pruning calls performed by iterative pruning is just a combination of the various masks applied in sequence. The combination of a new mask with the old mask is handled by the ```PruningContainer```'s ```compute_mask``` method.\n",
    "\n",
    "We can see that The corresponding hook will now be of type torch.nn.utils.prune.PruningContainer, and will store the history of pruning applied to the weight parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pursuant-resolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.nn.utils.prune.PruningContainer object at 0x0000014D8B8B5610>\n",
      "[<torch.nn.utils.prune.L1Unstructured object at 0x0000014DBF2F2F70>, <torch.nn.utils.prune.L1Unstructured object at 0x0000014DBE2DE670>, <torch.nn.utils.prune.L1Unstructured object at 0x0000014D8B8B56D0>, <torch.nn.utils.prune.L1Unstructured object at 0x0000014DBE2D2850>, <torch.nn.utils.prune.L1Unstructured object at 0x0000014DBE2D28E0>]\n"
     ]
    }
   ],
   "source": [
    "for hook in module._forward_pre_hooks.values():\n",
    "    print(hook)\n",
    "    if hook._tensor_name == \"weight\":  # select out the correct hook\n",
    "        break\n",
    "\n",
    "print(list(hook))  # pruning history in the container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-meditation",
   "metadata": {},
   "source": [
    "### Remove Pruning Re-parametrization \n",
    "\n",
    "To make the pruning permanent (i.e.: zero out the parameters), we need to remove the re-parametrization in terms\n",
    "of ``weight_orig`` and ``weight_mask``, and remove the ``forward_pre_hook``, we can use the ``remove`` functionality from ``torch.nn.utils.prune``. \n",
    "\n",
    "What we are doing here is reassigning the attribute ``weight`` to the model parameters, in its pruned version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "spread-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-parametrization for single pruning\n",
    "prune.remove(module, 'weight')\n",
    "\n",
    "# re-parametrization for global pruning\n",
    "for module, parameter in modules_to_prune:\n",
    "    prune.remove(module, parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-kitty",
   "metadata": {},
   "source": [
    "### Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "laden-tribe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model vs single pruned model:\n",
      "Comparing size of models\n",
      "model:  original_model  \t Size (KB): 8935.103\n",
      "model:  locally_pruned_model  \t Size (KB): 8935.103\n",
      "1.00 times smaller\n",
      "\n",
      "Comparing latency of models\n",
      "model:  original_model  \t prediction time: 0.007002353668212891s\n",
      "model:  locally_pruned_model  \t prediction time: 0.0019996166229248047s\n",
      "\n",
      "Comparing accuracy of models\n",
      "model:  original_model  \t Test Accuracy: 0.74\n",
      "model:  locally_pruned_model  \t Test Accuracy: 0.76\n",
      "\n",
      "\n",
      "Original model vs globally pruned model:\n",
      "Comparing size of models\n",
      "model:  original_model  \t Size (KB): 8935.103\n",
      "model:  globally_pruned_model  \t Size (KB): 8935.103\n",
      "1.00 times smaller\n",
      "\n",
      "Comparing latency of models\n",
      "model:  original_model  \t prediction time: 0.002000570297241211s\n",
      "model:  globally_pruned_model  \t prediction time: 0.002000570297241211s\n",
      "\n",
      "Comparing accuracy of models\n",
      "model:  original_model  \t Test Accuracy: 0.74\n",
      "model:  globally_pruned_model  \t Test Accuracy: 0.73\n"
     ]
    }
   ],
   "source": [
    "INFERENCE_IMAGE_PATH = \"../../resources/data/fruits_image_classification/test/apple/image1.jpg\"\n",
    "TEST_DATASET_ROOTDIR = \"../../resources/data/fruits_image_classification/test\"\n",
    "\n",
    "# load image\n",
    "inference_image = load_image(INFERENCE_IMAGE_PATH)\n",
    "\n",
    "# load test dataset\n",
    "test_dataloader = load_dataset(TEST_DATASET_ROOTDIR)\n",
    "\n",
    "print(\"Original model vs single pruned model:\")\n",
    "compare_performance(original_model, single_pruned_model, \"original_model\", \"locally_pruned_model\", inference_image, test_dataloader)\n",
    "\n",
    "print(\"\\n\\nOriginal model vs globally pruned model:\")\n",
    "compare_performance(original_model, globally_pruned_model, \"original_model\", \"globally_pruned_model\", inference_image, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-bachelor",
   "metadata": {},
   "source": [
    "### Compare Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "western-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_global_sparsity(model):\n",
    "    return 100. * float(\n",
    "        torch.sum(model.conv1.weight == 0)\n",
    "        + torch.sum(model.conv2.weight == 0)\n",
    "        + torch.sum(model.fc1.weight == 0)\n",
    "        + torch.sum(model.fc2.weight == 0)\n",
    "        + torch.sum(model.fc3.weight == 0)\n",
    "    ) / float(\n",
    "        model.conv1.weight.nelement()\n",
    "        + model.conv2.weight.nelement()\n",
    "        + model.fc1.weight.nelement()\n",
    "        + model.fc2.weight.nelement()\n",
    "        + model.fc3.weight.nelement()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "neural-grant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global sparsity for unpruned model: 0.00%\n",
      "Global sparsity for locally pruned model: 0.02%\n",
      "Global sparsity for globally pruned model: 83.19%\n"
     ]
    }
   ],
   "source": [
    "print(\"Global sparsity for unpruned model: {:.2f}%\".format(calculate_global_sparsity(original_model)))\n",
    "print(\"Global sparsity for locally pruned model: {:.2f}%\".format(calculate_global_sparsity(single_pruned_model)))\n",
    "print(\"Global sparsity for globally pruned model: {:.2f}%\".format(calculate_global_sparsity(globally_pruned_model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-tender",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [Pruning for Neural Networks by Lei Mao](https://leimao.github.io/article/Neural-Networks-Pruning/)\n",
    "- [PyTorch Pruning by Lei Mao](https://leimao.github.io/blog/PyTorch-Pruning/)\n",
    "- [Pruning Tutorial by Pytorch](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deploy] *",
   "language": "python",
   "name": "conda-env-deploy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
