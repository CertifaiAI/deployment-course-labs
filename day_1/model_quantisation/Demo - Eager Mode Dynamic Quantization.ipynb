{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "antique-demand",
   "metadata": {},
   "source": [
    "![logo](../../picture/license_header_logo.png)\n",
    "**Copyright (c) 2020-2021 CertifAI Sdn. Bhd.**\n",
    "\n",
    "This program is part of OSRFramework. You can redistribute it and/or modify\n",
    "it under the terms of the GNU Affero General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n",
    "GNU Affero General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Affero General Public License\n",
    "along with this program. If not, see http://www.gnu.org/licenses/.\n",
    "\n",
    "Authored by: [Jacklyn Lim](mailto:jacklyn.lim@certifai.ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-monte",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "concerned-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import download_model, download_dataset, load_model_state_dict, load_dataset, load_image, compare_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d15251",
   "metadata": {},
   "source": [
    "### Download Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6949afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model already exists, skipping download\n",
      "data already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "# model download\n",
    "MODEL_DOWNLOAD_PATH = 'https://s3.eu-central-1.wasabisys.com/certifai/deployment-training-labs/models/fruit_classifier_state_dict.pt'\n",
    "MODEL_STATE_DICT_PATH = '../../resources/model/'\n",
    "MODEL_FILENAME = 'fruits_image_classification.zip'\n",
    "\n",
    "# data download\n",
    "DATA_DOWNLOAD_PATH = \"https://s3.eu-central-1.wasabisys.com/certifai/deployment-training-labs/fruits_image_classification-20210604T123547Z-001.zip\"\n",
    "DATA_SAVE_PATH = \"../../resources/data/\"\n",
    "DATA_ZIP_FILENAME = \"fruits_image_classification.zip\"\n",
    "\n",
    "# download model\n",
    "download_model(MODEL_DOWNLOAD_PATH, MODEL_STATE_DICT_PATH, MODEL_FILENAME)\n",
    "\n",
    "# download dataset\n",
    "download_dataset(DATA_DOWNLOAD_PATH, DATA_SAVE_PATH, DATA_ZIP_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-sixth",
   "metadata": {},
   "source": [
    "### Load Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stunning-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # Note that the input of this layers is depending on your input image sizes\n",
    "        self.fc1 = nn.Linear(18496, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81f10c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFP32 Model: \u001b[0m\n",
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=18496, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load original model\n",
    "model_fp32 = Net()\n",
    "model_fp32 = load_model_state_dict(model_fp32, MODEL_STATE_DICT_PATH + MODEL_FILENAME)\n",
    "model_fp32.eval()\n",
    "\n",
    "# Print original model\n",
    "print(\"\\033[1mFP32 Model: \\033[0m\")\n",
    "print(model_fp32)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8381834",
   "metadata": {},
   "source": [
    "### Quantize Model Using Dynamic Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b46efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_quantization(model_fp32):\n",
    "    \"\"\" Returns a quantized model \"\"\"\n",
    "    # create a quantized model instance\n",
    "    model_int8 = torch.quantization.quantize_dynamic(\n",
    "        model_fp32,  # the original model\n",
    "        {torch.nn.Linear},  # a set of layers to dynamically quantize\n",
    "        dtype=torch.qint8)  # the target dtype for quantized weights\n",
    "\n",
    "    return model_int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "012c6e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINT8 Model: \u001b[0m\n",
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): DynamicQuantizedLinear(in_features=18496, out_features=120, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  (fc2): DynamicQuantizedLinear(in_features=120, out_features=84, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  (fc3): DynamicQuantizedLinear(in_features=84, out_features=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# quantising model dynamically\n",
    "model_int8 = dynamic_quantization(model_fp32)\n",
    "\n",
    "# Print quantized model\n",
    "print(\"\\033[1mINT8 Model: \\033[0m\")\n",
    "print(model_int8)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-equation",
   "metadata": {},
   "source": [
    "### Compare Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-payment",
   "metadata": {},
   "source": [
    "Since Pytorch quantization only supports CPU inference in the following backends: x86 and ARM for now, we need to place the model and image data on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "colored-vessel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing size of models\n",
      "model:  model_fp32  \t Size (KB): 8935.103\n",
      "model:  model_int8  \t Size (KB): 2247.599\n",
      "3.98 times smaller\n",
      "\n",
      "Comparing latency of models\n",
      "model:  model_fp32  \t prediction time: 0.004999399185180664s\n",
      "model:  model_int8  \t prediction time: 0.004001140594482422s\n",
      "\n",
      "Comparing accuracy of models\n",
      "model:  model_fp32  \t Test Accuracy: 0.74\n",
      "model:  model_int8  \t Test Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "INFERENCE_IMAGE_PATH = \"../../resources/data/fruits_image_classification/test/apple/image1.jpg\"\n",
    "TEST_DATASET_ROOTDIR = \"../../resources/data/fruits_image_classification/test\"\n",
    "   \n",
    "# load image\n",
    "inference_image = load_image(INFERENCE_IMAGE_PATH)\n",
    "\n",
    "# load test dataset\n",
    "test_dataloader = load_dataset(TEST_DATASET_ROOTDIR)\n",
    "\n",
    "# compare performance between original model and quantized model\n",
    "compare_performance(model_fp32, model_int8, \"model_fp32\", \"model_int8\", inference_image, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deploy] *",
   "language": "python",
   "name": "conda-env-deploy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
