{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nominated-affair",
   "metadata": {},
   "source": [
    "![logo](../../picture/license_header_logo.png)\n",
    "**Copyright (c) 2020-2021 CertifAI Sdn. Bhd.**\n",
    "\n",
    "This program is part of OSRFramework. You can redistribute it and/or modify\n",
    "it under the terms of the GNU Affero General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n",
    "GNU Affero General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Affero General Public License\n",
    "along with this program. If not, see http://www.gnu.org/licenses/.\n",
    "\n",
    "Authored by: [Jacklyn Lim](mailto:jacklyn.lim@certifai.ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-wireless",
   "metadata": {},
   "source": [
    "### Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14f820ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "from utils import download_model, download_dataset, load_model_state_dict, load_dataset, load_image, compare_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5104c3",
   "metadata": {},
   "source": [
    "### Download Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "875b6ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model already exists, skipping download\n",
      "data already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "# model download\n",
    "MODEL_DOWNLOAD_PATH = 'https://s3.eu-central-1.wasabisys.com/certifai/deployment-training-labs/models/fruit_classifier_state_dict.pt'\n",
    "MODEL_STATE_DICT_PATH = '../../resources/model/'\n",
    "MODEL_FILENAME = 'fruits_image_classification.zip'\n",
    "\n",
    "# data download\n",
    "DATA_DOWNLOAD_PATH = \"https://s3.eu-central-1.wasabisys.com/certifai/deployment-training-labs/fruits_image_classification-20210604T123547Z-001.zip\"\n",
    "DATA_SAVE_PATH = \"../../resources/data/\"\n",
    "DATA_ZIP_FILENAME = \"fruits_image_classification.zip\"\n",
    "\n",
    "# download model\n",
    "download_model(MODEL_DOWNLOAD_PATH, MODEL_STATE_DICT_PATH, MODEL_FILENAME)\n",
    "\n",
    "# download dataset\n",
    "download_dataset(DATA_DOWNLOAD_PATH, DATA_SAVE_PATH, DATA_ZIP_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519d4eff",
   "metadata": {},
   "source": [
    "### Load Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b5c3f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # use independent ReLUs for layer fusion.\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "        self.relu4 = torch.nn.ReLU()\n",
    "        # Note that the input of this layers is depending on your input image sizes\n",
    "        self.fc1 = nn.Linear(18496, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "907bf71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFP32 Model: \u001b[0m\n",
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (relu2): ReLU()\n",
      "  (relu3): ReLU()\n",
      "  (relu4): ReLU()\n",
      "  (fc1): Linear(in_features=18496, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load original model\n",
    "model_fp32 = Net()\n",
    "model_fp32 = load_model_state_dict(model_fp32, MODEL_STATE_DICT_PATH + MODEL_FILENAME)\n",
    "model_fp32.eval()\n",
    "\n",
    "# Print original model\n",
    "print(\"\\033[1mFP32 Model: \\033[0m\")\n",
    "print(model_fp32)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-entry",
   "metadata": {},
   "source": [
    "### Dynamic Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "varied-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_quantization(model_fp32):\n",
    "    \"\"\" Returns a quantized model \"\"\"\n",
    "    pass\n",
    "    ############ Enter your code here ############\n",
    "    \n",
    "    ############ Enter your code here ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c077d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINT8 Model: \u001b[0m\n",
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (relu2): ReLU()\n",
      "  (relu3): ReLU()\n",
      "  (relu4): ReLU()\n",
      "  (fc1): DynamicQuantizedLinear(in_features=18496, out_features=120, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  (fc2): DynamicQuantizedLinear(in_features=120, out_features=84, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  (fc3): DynamicQuantizedLinear(in_features=84, out_features=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# quantising model dynamically\n",
    "model_int8 = dynamic_quantization(model_fp32)\n",
    "\n",
    "# Print quantized model\n",
    "print(\"\\033[1mINT8 Model: \\033[0m\")\n",
    "print(model_int8)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-statement",
   "metadata": {},
   "source": [
    "#### Perform Inference and Compare Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "occupied-spanish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing size of models\n",
      "model:  model_fp32  \t Size (KB): 8935.167\n",
      "model:  model_int8  \t Size (KB): 2247.663\n",
      "3.98 times smaller\n",
      "\n",
      "Comparing latency of models\n",
      "model:  model_fp32  \t prediction time: 0.004000425338745117s\n",
      "model:  model_int8  \t prediction time: 0.00299835205078125s\n",
      "\n",
      "Comparing accuracy of models\n",
      "model:  model_fp32  \t Test Accuracy: 0.74\n",
      "model:  model_int8  \t Test Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "INFERENCE_IMAGE_PATH = \"../../resources/data/fruits_image_classification/test/apple/image1.jpg\"\n",
    "TEST_DATASET_ROOTDIR = \"../../resources/data/fruits_image_classification/test\"\n",
    "\n",
    "# load image\n",
    "inference_image = load_image(INFERENCE_IMAGE_PATH)\n",
    "\n",
    "# load test dataset\n",
    "test_dataloader = load_dataset(TEST_DATASET_ROOTDIR)\n",
    "\n",
    "# compare performance between original model and quantized model\n",
    "compare_performance(model_fp32, model_int8, \"model_fp32\", \"model_int8\", inference_image, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-documentation",
   "metadata": {},
   "source": [
    "### Static Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4afccc",
   "metadata": {},
   "source": [
    "#### Fuse Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e702a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_layers(model, fusion_layers_list):\n",
    "    \"\"\"\n",
    "    A function to fuse specified layers in the model.\n",
    "\n",
    "    Args:\n",
    "        model (Net): model to be quantized\n",
    "        fusion_layers_list (list): a list of layers to be fused\n",
    "\n",
    "    Returns:\n",
    "        Model with fused layer\n",
    "    \"\"\"\n",
    "    pass\n",
    "    ############ Enter your code here ############\n",
    "    \n",
    "    ############ Enter your code here ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "818d7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuse layers\n",
    "model_fp32_fused = fuse_layers(model_fp32, [['conv1', 'relu1'], ['conv2', 'relu2'], ['fc1', 'relu3'], ['fc2', 'relu4']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-worse",
   "metadata": {},
   "source": [
    "##### Check if Fused Model Outputs the Same as the Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "overhead-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_equivalence(model_fp32, model_fp32_fused, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32)):\n",
    "    \"\"\"\n",
    "    Check if the fused model has approximately the same output as the original model. \n",
    "    \n",
    "    Args:\n",
    "        model_fp32 (Net): Original model\n",
    "        model_fp32_fused (Net): Fused model\n",
    "        device (String): Inference device (CPU)\n",
    "        rtol (float): The relative tolerance parameter (see numpy documentation for np.allclose)\n",
    "        atol (float): The absolute tolerance parameter (see numpy documentation for np.allclose)\n",
    "        num_tests (int): Number of iterations to test the equaivalance of both models\n",
    "        input_size (tuple): image size\n",
    "\n",
    "    Returns:\n",
    "        True if two arrays are element-wise equal within a tolerance, otherwise False   \n",
    "    \"\"\"\n",
    "    model_fp32.to(device)\n",
    "    model_fp32_fused.to(device)\n",
    "\n",
    "    for _ in range(num_tests):\n",
    "        x = torch.rand(size=input_size).to(device)\n",
    "        y_model_fp32 = model_fp32(x).detach().cpu().numpy()\n",
    "        y_model_fp32_fused = model_fp32_fused(x).detach().cpu().numpy()\n",
    "        \n",
    "        # Returns True if two arrays are element-wise equal within a tolerance\n",
    "        if np.allclose(a=y_model_fp32, b=y_model_fp32_fused, rtol=rtol, atol=atol, equal_nan=False) == False: \n",
    "            print(\"Model equivalence test sample failed: \")\n",
    "            print(y_model_fp32)\n",
    "            print(y_model_fp32_fused)\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d579b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if fused model outputs the same as the original model\n",
    "assert check_model_equivalence(model_fp32=model_fp32,\n",
    "                               model_fp32_fused=model_fp32_fused,\n",
    "                               device=\"cpu\", rtol=1e-03, atol=1e-06, num_tests=100,\n",
    "                               input_size=(1, 3, 150, 150)), \"Fused model is not equivalent to the original model!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d97df17",
   "metadata": {},
   "source": [
    "#### Define Quantized Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a4cbf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedNet(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        pass\n",
    "        ############ Enter your code here ############\n",
    "\n",
    "        ############ Enter your code here ############\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "        ############ Enter your code here ############\n",
    "\n",
    "        ############ Enter your code here ############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de34f7f",
   "metadata": {},
   "source": [
    "Create a quantized model after fusing layers\n",
    "\n",
    "Note: this step usually has to come after the layer fusion if there is a BatchNorm layer since there is no quantized layer implementation for a single batch normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9419430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = QuantizedNet(model_fp32=model_fp32_fused)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-dominican",
   "metadata": {},
   "source": [
    "#### Set Backend to Run Quantized Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "revolutionary-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_operatoring_backend(model):\n",
    "    \"\"\"\n",
    "    A function to set backend to run the quantized operators \n",
    "\n",
    "    Args:\n",
    "        model (Net): model to be quantized\n",
    "\n",
    "    Returns:\n",
    "        Model with set qconfig (engine used for quantized computations)\n",
    "    \"\"\"\n",
    "    pass\n",
    "    ############ Enter your code here ############\n",
    "    \n",
    "    ############ Enter your code here ############\n",
    "\n",
    "# set backend\n",
    "quantized_model = set_operatoring_backend(quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3f1dec",
   "metadata": {},
   "source": [
    "#### Calibration With a Representative Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55d33b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration(model, dataloader, device=\"cpu\"):\n",
    "    \"\"\" Returns calibrated model\"\"\"\n",
    "    pass\n",
    "    ############ Enter your code here ############\n",
    "    \n",
    "    ############ Enter your code here ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87b09301",
   "metadata": {},
   "outputs": [],
   "source": [
    "CALIBRATION_DATASET_ROOTDIR = \"../../resources/data/fruits_image_classification/train\"\n",
    "\n",
    "# load calibration dataset\n",
    "calibration_dataloader = load_dataset(CALIBRATION_DATASET_ROOTDIR)\n",
    "\n",
    "# prepare a copy of the model for the calibration step\n",
    "quantized_model = torch.quantization.prepare(quantized_model, inplace=True)\n",
    "\n",
    "# calibration\n",
    "calibrated_model = calibration(quantized_model, calibration_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-discipline",
   "metadata": {},
   "source": [
    "#### Convert Calibrated Model to a Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "elder-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_quantized_model(calibrated_model):\n",
    "    \"\"\" Returns a quantized int8 model\"\"\"\n",
    "    pass\n",
    "    ############ Enter your code here ############\n",
    "    \n",
    "    ############ Enter your code here ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28ae6aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINT8 Model: \u001b[0m\n",
      "QuantizedNet(\n",
      "  (quant): Quantize(scale=tensor([0.0186]), zero_point=tensor([114]), dtype=torch.quint8)\n",
      "  (model_fp32): Net(\n",
      "    (conv1): QuantizedConvReLU2d(3, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.05027412995696068, zero_point=0)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): QuantizedConvReLU2d(6, 16, kernel_size=(5, 5), stride=(1, 1), scale=0.09794529527425766, zero_point=0)\n",
      "    (relu1): Identity()\n",
      "    (relu2): Identity()\n",
      "    (relu3): Identity()\n",
      "    (relu4): Identity()\n",
      "    (fc1): QuantizedLinearReLU(in_features=18496, out_features=120, scale=0.5772956609725952, zero_point=0, qscheme=torch.per_tensor_affine)\n",
      "    (fc2): QuantizedLinearReLU(in_features=120, out_features=84, scale=0.28039851784706116, zero_point=0, qscheme=torch.per_tensor_affine)\n",
      "    (fc3): QuantizedLinear(in_features=84, out_features=3, scale=0.545403003692627, zero_point=160, qscheme=torch.per_tensor_affine)\n",
      "  )\n",
      "  (dequant): DeQuantize()\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to a quantized model\n",
    "model_int8 = convert_to_quantized_model(calibrated_model)\n",
    "model_int8.eval()\n",
    "\n",
    "# Print quantized model\n",
    "print(\"\\033[1mINT8 Model: \\033[0m\")\n",
    "print(model_int8)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fae8a5",
   "metadata": {},
   "source": [
    "#### Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d5d4bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCOMPARING PERFORMANCE... \u001b[0m\n",
      "Comparing size of models\n",
      "model:  model_fp32  \t Size (KB): 8935.167\n",
      "model:  model_int8  \t Size (KB): 2241.183\n",
      "3.99 times smaller\n",
      "\n",
      "Comparing latency of models\n",
      "model:  model_fp32  \t prediction time: 0.0029985904693603516s\n",
      "model:  model_int8  \t prediction time: 0.006000041961669922s\n",
      "\n",
      "Comparing accuracy of models\n",
      "model:  model_fp32  \t Test Accuracy: 0.74\n",
      "model:  model_int8  \t Test Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "INFERENCE_IMAGE_PATH = \"../../resources/data/fruits_image_classification/test/apple/image1.jpg\"\n",
    "TEST_DATASET_ROOTDIR = \"../../resources/data/fruits_image_classification/test\"\n",
    "\n",
    "# COMPARING PERFORMANCE \n",
    "print(\"\\033[1mCOMPARING PERFORMANCE... \\033[0m\")\n",
    "# load image\n",
    "inference_image = load_image(INFERENCE_IMAGE_PATH)\n",
    "\n",
    "# load test dataset\n",
    "test_dataloader = load_dataset(TEST_DATASET_ROOTDIR)\n",
    "\n",
    "# compare performance between original model and quantized model\n",
    "compare_performance(model_fp32, model_int8, \"model_fp32\", \"model_int8\", inference_image, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134de6dc",
   "metadata": {},
   "source": [
    "#### Save Torchscript Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6245927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torchscript_model(model, model_dir, model_filename):\n",
    "    pass\n",
    "    ############ Enter your code here ############\n",
    "    \n",
    "    ############ Enter your code here ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "927831b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = \"../generated_model\"\n",
    "TORCHSCRIPT_MODEL_FILENAME = \"eagermode_static_quantized_model.pt\"\n",
    "\n",
    "save_torchscript_model(model_int8, MODEL_SAVE_PATH, TORCHSCRIPT_MODEL_FILENAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deploy] *",
   "language": "python",
   "name": "conda-env-deploy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
