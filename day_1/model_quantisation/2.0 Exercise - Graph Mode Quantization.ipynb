{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "collectible-innocent",
   "metadata": {},
   "source": [
    "![logo](../../picture/license_header_logo.png)\n",
    "**Copyright (c) 2020-2021 CertifAI Sdn. Bhd.**\n",
    "\n",
    "This program is part of OSRFramework. You can redistribute it and/or modify\n",
    "it under the terms of the GNU Affero General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n",
    "GNU Affero General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Affero General Public License\n",
    "along with this program. If not, see http://www.gnu.org/licenses/.\n",
    "\n",
    "Authored by: [Jacklyn Lim](mailto:jacklyn.lim@certifai.ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-spell",
   "metadata": {},
   "source": [
    "### Motivation of Doing Graph Mode Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-emergency",
   "metadata": {},
   "source": [
    "By default, PyTorch uses eager mode computation, hence when we quantise our model, we are also using an eager mode approach. However, the quantization process is highly manual especially in the following aspects:\n",
    "- Explicitly adding in QuantStub and DeQuantStub when defining model architecture\n",
    "- Having to explicitly specify the layers to be fused during layers fusion\n",
    "- Lack of support for `torch.nn.functionals` (functional.conv2d and functional.linear would not get quantized)\n",
    "- Having to replace operations that require special handling such as `torch.cat` and `torch.cat` with `nn.quantized.FloatFunctional`\n",
    "\n",
    "Here is where Torchscript comes to rescue. Torchscript records its definitions in an Intermediate Representation (or IR), commonly referred to in Deep learning as a graph. Pytorch also provides quantization methods for Torchscript models. \n",
    "\n",
    "In graph mode, quantization is achieved by module and graph manipulations. It is able to automatically figure out things like which modules to fuse and where to insert observer calls, quantize/dequantize functions etc., hence the whole quantization process can be automated.\n",
    "\n",
    "Advantages of Graph Mode Quantization are:\n",
    "\n",
    "- Simple quantization flow, minimal manual steps\n",
    "- Unlocks the possibility of doing higher level optimizations like automatic precision selection\n",
    "\n",
    "Note: \n",
    "Graph Mode Quantization is still a very new feature (Available since Pytorch 1.8.0), so do expect changes in how it's implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-collaboration",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import download_model, download_dataset, load_model_state_dict, load_dataset, load_image, compare_performance\n",
    "\n",
    "# for Graph Mode Quantization\n",
    "from torch.quantization import quantize_dynamic_jit, per_channel_dynamic_qconfig, get_default_qconfig, quantize_jit\n",
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dd054c",
   "metadata": {},
   "source": [
    "### Download Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f2211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model download\n",
    "MODEL_DOWNLOAD_PATH = 'https://s3.eu-central-1.wasabisys.com/certifai/deployment-training-labs/models/fruit_classifier_state_dict.pt'\n",
    "MODEL_STATE_DICT_PATH = '../../resources/model/'\n",
    "MODEL_FILENAME = 'fruits_image_classification.zip'\n",
    "\n",
    "# data download\n",
    "DATA_DOWNLOAD_PATH = \"https://s3.eu-central-1.wasabisys.com/certifai/deployment-training-labs/fruits_image_classification-20210604T123547Z-001.zip\"\n",
    "DATA_SAVE_PATH = \"../../resources/data/\"\n",
    "DATA_ZIP_FILENAME = \"fruits_image_classification.zip\"\n",
    "\n",
    "# download model\n",
    "download_model(MODEL_DOWNLOAD_PATH, MODEL_STATE_DICT_PATH, MODEL_FILENAME)\n",
    "\n",
    "# download dataset\n",
    "download_dataset(DATA_DOWNLOAD_PATH, DATA_SAVE_PATH, DATA_ZIP_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546fe249",
   "metadata": {},
   "source": [
    "### Load Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c21076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # Note that the input of this layers is depending on your input image sizes\n",
    "        self.fc1 = nn.Linear(18496, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257fa7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original model\n",
    "model_fp32 = Net()\n",
    "model_fp32 = load_model_state_dict(model_fp32, MODEL_STATE_DICT_PATH + MODEL_FILENAME)\n",
    "model_fp32.eval()\n",
    "\n",
    "# Print original model\n",
    "print(\"\\033[1mFP32 Model: \\033[0m\")\n",
    "print(model_fp32)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-exploration",
   "metadata": {},
   "source": [
    "### Dynamic Quantization\n",
    "\n",
    "Reference: https://pytorch.org/tutorials/prototype/graph_mode_dynamic_bert_tutorial.html#quantizing-bert-model-with-graph-mode-quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a02933",
   "metadata": {},
   "source": [
    "#### Convert Original Model to Torchscript model\n",
    "\n",
    "We have to convert the original Pytorch model to a Torchscript model since the input for Graph Mode Quantization is a Torchscript model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac70815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# script model (convert to torchscript model)\n",
    "script_model = torch.jit.script(model_fp32).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81da7a1e",
   "metadata": {},
   "source": [
    "#### Quantize Model\n",
    "\n",
    "Using per-channel quantization which helps improving the final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba6ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task: Perform dynamic quantization using the quantize_dynamic_jit API\n",
    "\"\"\"\n",
    "############ Enter your code here ############\n",
    "\n",
    "############ Enter your code here ############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-injury",
   "metadata": {},
   "source": [
    "#### Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_IMAGE_PATH = \"../../resources/data/fruits_image_classification/test/apple/image1.jpg\"\n",
    "TEST_DATASET_ROOTDIR = \"../../resources/data/fruits_image_classification/test\"\n",
    "    \n",
    "# load image\n",
    "inference_image = load_image(INFERENCE_IMAGE_PATH)\n",
    "\n",
    "# load test dataset\n",
    "test_dataloader = load_dataset(TEST_DATASET_ROOTDIR)\n",
    "\n",
    "compare_performance(model_fp32, quantized_model, \"model_fp32\", \"quantized_model\", inference_image, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-press",
   "metadata": {},
   "source": [
    "### Static Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664bd0d4",
   "metadata": {},
   "source": [
    "#### Convert Original Model to Torchscript model\n",
    "\n",
    "We have to convert the original Pytorch model to a Torchscript model since the input for Graph Mode Quantization is a Torchscript model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d3b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# script model (convert to torchscript model)\n",
    "script_model = torch.jit.script(model_fp32).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-metro",
   "metadata": {},
   "source": [
    "#### Specify How to Quantize the Model With `qconfig_dict` (Including Specifying the Backend) \n",
    "\n",
    "Please refer to [Pytorch guide on setting qconfig](https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html#specify-how-to-quantize-the-model-with-qconfig-dict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_qconfig_dict(backend, config):\n",
    "    \"\"\"\n",
    "    A function to set backend to run the quantized operators \n",
    "\n",
    "    Parameters:\n",
    "    model (Net): model to be quantized\n",
    "\n",
    "    Returns:\n",
    "    Model with set qconfig (engine used for quantized computations)\n",
    "    \"\"\"\n",
    "    qconfig = get_default_qconfig(backend)\n",
    "    qconfig_dict = {config: qconfig}\n",
    "    return qconfig_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f33621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify how to quantize the model with qconfig_dict (including specifying the backend)\n",
    "qconfig_dict = set_qconfig_dict(\"qnnpack\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-retailer",
   "metadata": {},
   "source": [
    "#### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration(model, dataloader, device=\"cpu\"):\n",
    "    \"\"\" Returns calibrated model\"\"\"\n",
    "\n",
    "    pass\n",
    "    ############ Enter your code here ############\n",
    "    \n",
    "    ############ Enter your code here ############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec6ca6",
   "metadata": {},
   "source": [
    "For Graph Mode Quantization, instead of the `torch.quantization.prepare` function we will make use of the  `prepare_fx` function from the `torch.quantization.quantize_fx` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5166cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "CALIBRATION_DATASET_ROOTDIR = \"../../resources/data/fruits_image_classification/train\"\n",
    "\n",
    "# load calibration dataset\n",
    "calibration_dataloader = load_dataset(CALIBRATION_DATASET_ROOTDIR)\n",
    "\n",
    "# prepare model for calibration - including fuse modules and insert observers\n",
    "prepared_model = prepare_fx(model_fp32, qconfig_dict)\n",
    "\n",
    "# calibrate model\n",
    "calibrated_model = calibration(prepared_model, calibration_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-magic",
   "metadata": {},
   "source": [
    "#### Convert Calibrated Model to a Quantized Model\n",
    "\n",
    "Hint: For Graph Mode Quantization, instead of the `torch.quantization.convert` function we will make use of the `convert_fx` function from the torch.quantization.quantize_fx module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_quantized_model(calibrated_model):\n",
    "    \"\"\" Returns a quantized int8 model \"\"\"\n",
    "    pass\n",
    "    ############ Enter your code here ############\n",
    "    \n",
    "    ############ Enter your code here ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f7b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a quantized model\n",
    "model_int8 = convert_to_quantized_model(calibrated_model)\n",
    "model_int8.eval()\n",
    "\n",
    "# Print quantized model\n",
    "print(\"\\033[1mINT8 Model: \\033[0m\")\n",
    "print(model_int8)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-franchise",
   "metadata": {},
   "source": [
    "#### Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_IMAGE_PATH = \"../../resources/data/fruits_image_classification/test/apple/image1.jpg\"\n",
    "TEST_DATASET_ROOTDIR = \"../../resources/data/fruits_image_classification/test\"\n",
    "\n",
    "# COMPARING PERFORMANCE \n",
    "print(\"\\033[1mCOMPARING PERFORMANCE... \\033[0m\")\n",
    "\n",
    "# load image\n",
    "inference_image = load_image(INFERENCE_IMAGE_PATH)\n",
    "\n",
    "# load test dataset\n",
    "test_dataloader = load_dataset(TEST_DATASET_ROOTDIR)\n",
    "\n",
    "compare_performance(model_fp32, model_int8, \"model_fp32\", \"quantized_model\", inference_image, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5b6af2",
   "metadata": {},
   "source": [
    "#### Save Torchscript Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb046373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torchscript_model(model, model_dir, model_filename):\n",
    "    print(\"\\nSaving model...\")\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    torch.jit.save(torch.jit.script(model), model_filepath)\n",
    "    print(\"Model saved in {}\".format(model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39245600",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = \"../generated_model\"\n",
    "TORCHSCRIPT_MODEL_FILENAME = \"graphmode_static_quantized_model.pt\"\n",
    "\n",
    "save_torchscript_model(model_int8, MODEL_SAVE_PATH, TORCHSCRIPT_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-category",
   "metadata": {},
   "source": [
    "### Additional Notes\n",
    "\n",
    "Since graph model quantization automatically figures out things like which modules to fuse, it tries to quantize as many layers as possible, in contrast with manually specifying the layers to quantize. This is why we can see that the quantized model size is significantly smaller here.\n",
    "\n",
    "As mentioned previously, Pytorch Graph Mode Quantization is currently using the FX Graph Mode Quantization module which is still a very new feature (Available since Pytorch 1.8.0), so do expect changes in how it's implemented.\n",
    "\n",
    "Nonetheless from how easy it is to use and the performance improvement especially the model size, it is a very exciting feature to look forward to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-camping",
   "metadata": {},
   "source": [
    "### Extra Reading Materials\n",
    "- https://pytorch.org/docs/stable/quantization-support.html#torch-quantization\n",
    "- https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html\n",
    "- https://www.kaggle.com/okeaditya/what-s-new-in-pytorch-1-6\n",
    "- https://zhuanlan.zhihu.com/p/349019936"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deploy] *",
   "language": "python",
   "name": "conda-env-deploy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
