{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "addressed-mountain",
   "metadata": {},
   "source": [
    "![logo](../../picture/license_header_logo.png)\n",
    "**Copyright (c) 2020-2021 CertifAI Sdn. Bhd.**\n",
    "\n",
    "This program is part of OSRFramework. You can redistribute it and/or modify\n",
    "it under the terms of the GNU Affero General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n",
    "GNU Affero General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Affero General Public License\n",
    "along with this program. If not, see http://www.gnu.org/licenses/.\n",
    "\n",
    "Authored by: [Jacklyn Lim](mailto:jacklyn.lim@certifai.ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-roots",
   "metadata": {},
   "source": [
    "### Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14f820ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "from utils import download_model, download_dataset, load_model_state_dict, load_dataset, load_image, compare_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adfaee6",
   "metadata": {},
   "source": [
    "### Download Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35bec10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model already exists, skipping download\n",
      "data already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "# model download\n",
    "MODEL_DOWNLOAD_PATH = 'https://s3.eu-central-1.wasabisys.com/certifai/deployment-training-labs/models/fruit_classifier_state_dict.pt'\n",
    "MODEL_STATE_DICT_PATH = '../../resources/model/'\n",
    "MODEL_FILENAME = 'fruits_image_classification.zip'\n",
    "\n",
    "# data download\n",
    "DATA_DOWNLOAD_PATH = \"https://s3.eu-central-1.wasabisys.com/certifai/deployment-training-labs/fruits_image_classification-20210604T123547Z-001.zip\"\n",
    "DATA_SAVE_PATH = \"../../resources/data/\"\n",
    "DATA_ZIP_FILENAME = \"fruits_image_classification.zip\"\n",
    "\n",
    "# download model\n",
    "download_model(MODEL_DOWNLOAD_PATH, MODEL_STATE_DICT_PATH, MODEL_FILENAME)\n",
    "\n",
    "# download dataset\n",
    "download_dataset(DATA_DOWNLOAD_PATH, DATA_SAVE_PATH, DATA_ZIP_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519d4eff",
   "metadata": {},
   "source": [
    "### Load Original Model\n",
    "\n",
    "Up until today (June 2021), Pytorch quantization only supports `torch.nn.modules` and not `torch.nn.functionals`. Hence we cant use the Functional API while defining our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b5c3f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # use independent ReLUs for layer fusion.\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "        self.relu4 = torch.nn.ReLU()\n",
    "        # Note that the input of this layers is depending on your input image sizes\n",
    "        self.fc1 = nn.Linear(18496, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c4a91a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFP32 Model: \u001b[0m\n",
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (relu2): ReLU()\n",
      "  (relu3): ReLU()\n",
      "  (relu4): ReLU()\n",
      "  (fc1): Linear(in_features=18496, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load original model\n",
    "model_fp32 = Net()\n",
    "model_fp32 = load_model_state_dict(model_fp32, MODEL_STATE_DICT_PATH + MODEL_FILENAME)\n",
    "model_fp32.eval()\n",
    "\n",
    "# Print original model\n",
    "print(\"\\033[1mFP32 Model: \\033[0m\")\n",
    "print(model_fp32)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4afccc",
   "metadata": {},
   "source": [
    "### Fuse Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0472ba0",
   "metadata": {},
   "source": [
    "Operations/Modules fusions are usually also performed in static quantizations to combine operations/modules into a single module to obtain higher accuracy and performance. \n",
    "\n",
    "- Common fusions include `conv + relu` and `conv + batchnorm + relu`\n",
    "- Sometimes, layer fusion is compulsory, since there are no quantized layer implementations corresponding to some floating point layers, such as `BatchNorm`\n",
    "\n",
    "Currently Pytorch only supports the following fusions:\n",
    "- `[Conv, Relu]`\n",
    "- `[Conv, BatchNorm]`\n",
    "- `[Conv, BatchNorm, Relu]`\n",
    "- `[Linear, Relu]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e702a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_layers(model, fusion_layers_list):\n",
    "    \"\"\"\n",
    "    A function to fuse specified layers in the model.\n",
    "\n",
    "    Parameters:\n",
    "    model (Net): model to be quantized\n",
    "    fusion_layers_list (list): a list of layers to be fused\n",
    "\n",
    "    Returns:\n",
    "    Model with fused layer\n",
    "    \"\"\"\n",
    "    return torch.quantization.fuse_modules(model, fusion_layers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91dec657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFused FP32 Model: \u001b[0m\n",
      "Net(\n",
      "  (conv1): ConvReLU2d(\n",
      "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): ConvReLU2d(\n",
      "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (relu1): Identity()\n",
      "  (relu2): Identity()\n",
      "  (relu3): Identity()\n",
      "  (relu4): Identity()\n",
      "  (fc1): LinearReLU(\n",
      "    (0): Linear(in_features=18496, out_features=120, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): LinearReLU(\n",
      "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc3): Linear(in_features=84, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fuse layers\n",
    "model_fp32_fused = fuse_layers(model_fp32, [['conv1', 'relu1'], ['conv2', 'relu2'], ['fc1', 'relu3'], ['fc2', 'relu4']])\n",
    "# Print fused model\n",
    "print(\"\\033[1mFused FP32 Model: \\033[0m\")\n",
    "print(model_fp32_fused)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-worse",
   "metadata": {},
   "source": [
    "#### Check if Fused Model Outputs the Same as the Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "overhead-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_equivalence(model_fp32, model_fp32_fused, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32)):\n",
    "    \"\"\"\n",
    "    Check if the fused model has approximately the same output as the original model. \n",
    "    \n",
    "    Args:\n",
    "        model_fp32 (Net): Original model\n",
    "        model_fp32_fused (Net): Fused model\n",
    "        device (String): Inference device (CPU)\n",
    "        rtol (float): The relative tolerance parameter (see numpy documentation for np.allclose)\n",
    "        atol (float): The absolute tolerance parameter (see numpy documentation for np.allclose)\n",
    "        num_tests (int): Number of iterations to test the equaivalance of both models\n",
    "        input_size (tuple): image size\n",
    "\n",
    "    Returns:\n",
    "        True if two arrays are element-wise equal within a tolerance, otherwise False   \n",
    "    \"\"\"\n",
    "    model_fp32.to(device)\n",
    "    model_fp32_fused.to(device)\n",
    "\n",
    "    for _ in range(num_tests):\n",
    "        x = torch.rand(size=input_size).to(device)\n",
    "        y_model_fp32 = model_fp32(x).detach().cpu().numpy()\n",
    "        y_model_fp32_fused = model_fp32_fused(x).detach().cpu().numpy()\n",
    "        \n",
    "        # Returns True if two arrays are element-wise equal within a tolerance\n",
    "        if np.allclose(a=y_model_fp32, b=y_model_fp32_fused, rtol=rtol, atol=atol, equal_nan=False) == False: \n",
    "            print(\"Model equivalence test sample failed: \")\n",
    "            print(y_model_fp32)\n",
    "            print(y_model_fp32_fused)\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "978f02dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if fused model outputs the same as the original model\n",
    "assert check_model_equivalence(model_fp32=model_fp32,\n",
    "                               model_fp32_fused=model_fp32_fused,\n",
    "                               device=\"cpu\", rtol=1e-03, atol=1e-06, num_tests=100,\n",
    "                               input_size=(1, 3, 150, 150)), \"Fused model is not equivalent to the original model!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35557a6d",
   "metadata": {},
   "source": [
    "### Define Quantized Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0568a85d",
   "metadata": {},
   "source": [
    "Static quantization requires us to make modifications to our model code to enable quantization. We have to insert `torch.quantization.QuantStub` and `torch.quantization.DeQuantStub` layers at the beginning and end of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ab3b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedNet(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super().__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.model_fp32 = model_fp32\n",
    "        # DeQuantStub converts tensors from quantized to floating point\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee2c1fe",
   "metadata": {},
   "source": [
    "Create a quantized model after fusing layers\n",
    "\n",
    "Note: this step usually has to come after the layer fusion if there is a BatchNorm layer since there is no quantized layer implementation for a single batch normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ccbeab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = QuantizedNet(model_fp32=model_fp32_fused)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01563d6b",
   "metadata": {},
   "source": [
    "### Set Backend to Run Quantized Operators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4619341a",
   "metadata": {},
   "source": [
    "Today (June 2021), PyTorch supports the following backends for running quantized operators efficiently:\n",
    "\n",
    "- x86 CPUs with AVX2 support or higher (without AVX2 some operations have inefficient implementations)\n",
    "- ARM CPUs (typically found in mobile/embedded devices)\n",
    "\n",
    "PyTorch uses one of two purpose-built reduced-precision tensor matrix math libraries: FBGEMM on x86 ([repo](https://github.com/pytorch/FBGEMM)), QNNPACK ([repo](https://github.com/pytorch/pytorch/tree/169541871a7a6663cc86c3ab68501a62a5d8c67c/aten/src/ATen/native/quantized/cpu/qnnpack)) on ARM. \n",
    "\n",
    "It is necessary to ensure that qconfig and the engine used for quantized computations match the backend on which the model will be executed, in other words, if you are interested in quantizing a model to run on a mobile device, it is recommended to set the qconfig by calling:\n",
    "\n",
    "`model.qconfig = torch.quantization.get_default_qconfig('qnnpack')`\n",
    "\n",
    "On the other hand, if you are interested in quantizing a model to run on a server, you should set the qconfig by calling :\n",
    "\n",
    "`model.qconfig = torch.quantization.get_default_qconfig('fbgemm')`\n",
    "\n",
    "Read more about [FBGEMM](https://engineering.fb.com/2018/11/07/ml-applications/fbgemm/) and [QNNPACK](https://engineering.fb.com/2018/10/29/ml-applications/qnnpack/).\n",
    "\n",
    "Note: The backend to run the quantized operators doesn't seem to matter much upon converting the model into a Torchscript model, but let's stick to what is provided as a guideline in the Pytorch documentation to avoid any unforeseen incompatibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86880262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_operatoring_backend(model):\n",
    "    \"\"\"\n",
    "    A function to set backend to run the quantized operators \n",
    "\n",
    "    Args:\n",
    "        model (Net): model to be quantized\n",
    "\n",
    "    Returns:\n",
    "        Model with set qconfig (engine used for quantized computations)\n",
    "    \"\"\"\n",
    "    model.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dd3623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set backend\n",
    "quantized_model = set_operatoring_backend(quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3f1dec",
   "metadata": {},
   "source": [
    "### Calibration With a Representative Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3421f8cf",
   "metadata": {},
   "source": [
    "Static Quantization requires calibration with a representative dataset to determine optimal quantization parameters (scale & zero_point) for activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55d33b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration(model, dataloader, device=\"cpu\"):\n",
    "    \"\"\" Returns calibrated model\"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # pass to training loop to calibrate the model\n",
    "    for images, labels in dataloader:\n",
    "        inputs = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # output is not important since we are only doing this to calibrate the model\n",
    "        _ = model(inputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fe5a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CALIBRATION_DATASET_ROOTDIR = \"../../resources/data/fruits_image_classification/train\"\n",
    "\n",
    "# load calibration dataset\n",
    "calibration_dataloader = load_dataset(CALIBRATION_DATASET_ROOTDIR)\n",
    "\n",
    "# prepare a copy of the model for the calibration step\n",
    "quantized_model = torch.quantization.prepare(quantized_model, inplace=True)\n",
    "\n",
    "# calibration\n",
    "calibrated_model = calibration(quantized_model, calibration_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-discipline",
   "metadata": {},
   "source": [
    "### Convert Calibrated Model to a Quantized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-baking",
   "metadata": {},
   "source": [
    "Converts the model weights to int8 and replaces the operations with their quantized counterparts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "elder-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_quantized_model(calibrated_model):\n",
    "    \"\"\" Returns a quantized int8 model\"\"\"\n",
    "    return torch.quantization.convert(calibrated_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4d7c4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINT8 Model: \u001b[0m\n",
      "QuantizedNet(\n",
      "  (quant): Quantize(scale=tensor([0.0186]), zero_point=tensor([114]), dtype=torch.quint8)\n",
      "  (model_fp32): Net(\n",
      "    (conv1): QuantizedConvReLU2d(3, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.05027412995696068, zero_point=0)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): QuantizedConvReLU2d(6, 16, kernel_size=(5, 5), stride=(1, 1), scale=0.09794529527425766, zero_point=0)\n",
      "    (relu1): Identity()\n",
      "    (relu2): Identity()\n",
      "    (relu3): Identity()\n",
      "    (relu4): Identity()\n",
      "    (fc1): QuantizedLinearReLU(in_features=18496, out_features=120, scale=0.5772956609725952, zero_point=0, qscheme=torch.per_tensor_affine)\n",
      "    (fc2): QuantizedLinearReLU(in_features=120, out_features=84, scale=0.28039851784706116, zero_point=0, qscheme=torch.per_tensor_affine)\n",
      "    (fc3): QuantizedLinear(in_features=84, out_features=3, scale=0.545403003692627, zero_point=160, qscheme=torch.per_tensor_affine)\n",
      "  )\n",
      "  (dequant): DeQuantize()\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to a quantized model\n",
    "model_int8 = convert_to_quantized_model(calibrated_model)\n",
    "model_int8.eval()\n",
    "\n",
    "# Print quantized model\n",
    "print(\"\\033[1mINT8 Model: \\033[0m\")\n",
    "print(model_int8)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fae8a5",
   "metadata": {},
   "source": [
    "### Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d5d4bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCOMPARING PERFORMANCE... \u001b[0m\n",
      "Comparing size of models\n",
      "model:  model_fp32  \t Size (KB): 8935.167\n",
      "model:  model_int8  \t Size (KB): 2241.183\n",
      "3.99 times smaller\n",
      "\n",
      "Comparing latency of models\n",
      "model:  model_fp32  \t prediction time: 0.001993894577026367s\n",
      "model:  model_int8  \t prediction time: 0.0060024261474609375s\n",
      "\n",
      "Comparing accuracy of models\n",
      "model:  model_fp32  \t Test Accuracy: 0.74\n",
      "model:  model_int8  \t Test Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "INFERENCE_IMAGE_PATH = \"../../resources/data/fruits_image_classification/test/apple/image1.jpg\"\n",
    "TEST_DATASET_ROOTDIR = \"../../resources/data/fruits_image_classification/test\"    \n",
    "\n",
    "############ COMPARING PERFORMANCE ############\n",
    "print(\"\\033[1mCOMPARING PERFORMANCE... \\033[0m\")\n",
    "# load image\n",
    "inference_image = load_image(INFERENCE_IMAGE_PATH)\n",
    "\n",
    "# load test dataset\n",
    "test_dataloader = load_dataset(TEST_DATASET_ROOTDIR)\n",
    "\n",
    "# compare performance between original model and quantized model\n",
    "compare_performance(model_fp32, model_int8, \"model_fp32\",\n",
    "                    \"model_int8\", inference_image, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-methodology",
   "metadata": {},
   "source": [
    "### Additional Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-lyric",
   "metadata": {},
   "source": [
    "Compared to dynamic quantization, static quantization requires 2 extra steps (layers fusion & calibration), usually the additional steps of layers fusion and calibration helps speeding up the computation and takes much less of an accuracy hit compared to dynamic quantization since it gives the algorithm the opportunity to calibrate using real data all at once, instead of having to do so one-at-a-time at run time.\n",
    "\n",
    "As stated in the Pytorch documentation, one thing to take note of for static quantization is that since PyTorch makes use of the two purpose-built reduced-precision tensor matrix math libraries to run quantized operators ([FBGEMM](https://github.com/pytorch/FBGEMM) on x86), ([QNNPACK](https://github.com/pytorch/pytorch/tree/169541871a7a6663cc86c3ab68501a62a5d8c67c/aten/src/ATen/native/quantized/cpu/qnnpack) on ARM), hence the backend used for quantizing computations in static quantization <b>must be the same as the architecture as your deployment target</b> (eg: `qnnpack` for mobile deployment and `fbgemm` for hosting your model on a server). \n",
    "\n",
    "i.e.: If you are using FBGEMM, you must perform the calibration pass on an x86 CPU; if you are using QNNPACK, calibration needs to happen on an ARM CPU.\n",
    "\n",
    "However, after testing both quantized models using different tensor matrix math libraries which are then serialised in Torchscript models on a mobile device, it doesn't seem to matter much which library was used to quantize the model, but let's stick to what is provided as a guideline in the Pytorch documentation to avoid any unforeseen incompatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-devices",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [Introduction to Quantization on PyTorch by Pytorch](https://pytorch.org/blog/introduction-to-quantization-on-pytorch)\n",
    "- [Static Quantization With Eager Mode in Pytorch](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)\n",
    "- [A developer-friendly guide to model quantization with PyTorch by Spell](https://spell.ml/blog/pytorch-quantization-X8e7wBAAACIAHPhT)\n",
    "- [PyTorch Static Quantization by Lei Mao](https://leimao.github.io/blog/PyTorch-Static-Quantization/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deploy] *",
   "language": "python",
   "name": "conda-env-deploy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
